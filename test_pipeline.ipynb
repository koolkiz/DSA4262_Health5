{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from transformers import BertTokenizer\n",
    "import os\n",
    "from docx import Document\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and clean all clinical notes\n",
    "notes_path = \"data/notes\"\n",
    "\n",
    "def load_docx(file_path):\n",
    "    \"\"\"Load a .docx file and extract text\"\"\"\n",
    "    doc = Document(file_path)\n",
    "    text = \"\\n\".join([para.text for para in doc.paragraphs])\n",
    "    return text\n",
    "\n",
    "def clean_clinical_text(text):\n",
    "    \"\"\"Cleans clinical text for NLP processing.\"\"\"\n",
    "    text = \"\\n\".join([line.strip() for line in text.split(\"\\n\") if line.strip()])\n",
    "    text = text.lower()\n",
    "    text = text.split(\"\\n\", 1)[1]\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "    return text\n",
    "\n",
    "def load_clinical_notes(file_path):\n",
    "    notes = {}\n",
    "    for root, dirs, files in os.walk(file_path):\n",
    "        for file in files:\n",
    "            filename, file_extension = os.path.splitext(file)\n",
    "            if file.endswith(\".docx\"):\n",
    "                note = load_docx(os.path.join(root, file))\n",
    "                note = clean_clinical_text(note)\n",
    "                notes[filename] = note\n",
    "    return notes\n",
    "\n",
    "d = load_clinical_notes(notes_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_folder = 'data/images/all'\n",
    "notes_folder = 'data/notes'\n",
    "image_list = os.listdir(image_folder)\n",
    "\n",
    "def extract_image_id(image_path):\n",
    "    # image_path = 'P100_L_CM_MLO.jpg'\n",
    "    # Extract the image ID from the filename\n",
    "\n",
    "    # Assuming the image ID is the part before the first underscore\n",
    "    image_id = image_path.split('_')[0]\n",
    "    return image_id\n",
    "\n",
    "image_ids = [extract_image_id(image_path) for image_path in image_list]\n",
    "\n",
    "# create list of paths for all 503 images and their corresponding notes\n",
    "image_paths = [os.path.join(image_folder, image_path) for image_path in image_list]\n",
    "notes = [d[image_id] if image_id in d else None for image_id in image_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we have a list of image paths and their corresponding notes\n",
    "# Time to extract the corresponding labels from the CSV file\n",
    "\n",
    "# Length of image_paths, notes and labels = 503\n",
    "# Normal = 0 \n",
    "# Benign = 1\n",
    "# Malignant = 2\n",
    "\n",
    "# Load the CSV file\n",
    "csv_path = 'data/clinical_data.csv'\n",
    "df = pd.read_csv(csv_path)\n",
    "labels = []\n",
    "encode = {'Benign': 1, 'Malignant': 2, 'Normal': 0}\n",
    "\n",
    "for id in image_list:\n",
    "    c = id.split('.')[0]\n",
    "    \n",
    "    # extract row where column 'Image_name' = c and find pathology column\n",
    "    row = df[df['Image_name'] == c]\n",
    "    if not row.empty:\n",
    "        pathology = row['Pathology Classification/ Follow up'].values[0]\n",
    "        encode_pathology = encode.get(pathology, None)\n",
    "        if encode_pathology is not None:\n",
    "            labels.append(encode_pathology)\n",
    "        else:\n",
    "            # If the pathology is not in the encode dictionary, append None\n",
    "            print(f\"Pathology '{pathology}' not found in encoding dictionary for image {c}.\")\n",
    "    else:\n",
    "        print(c)\n",
    "        print(f\"No matching row found in CSV for image {c}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of image_paths: 503\n",
      "Length of notes: 503\n",
      "Length of labels: 503\n",
      "Normal:  206\n",
      "Benign:  128\n",
      "Malignant:  168\n"
     ]
    }
   ],
   "source": [
    "# Check if the lengths of image_paths, notes, and labels match\n",
    "print(f\"Length of image_paths: {len(image_paths)}\")\n",
    "print(f\"Length of notes: {len(notes)}\")\n",
    "print(f\"Length of labels: {len(labels)}\")\n",
    "\n",
    "print(\"Normal: \", labels.count(0))\n",
    "print(\"Benign: \", labels.count(1))\n",
    "print(\"Malignant: \", labels.count(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================\n",
    "#    Custom Dataset Class\n",
    "# ============================\n",
    "class CancerDataset(Dataset):\n",
    "    def __init__(self, image_paths, notes, labels, tokenizer, max_len, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.notes = notes\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Load Image\n",
    "        img_path = self.image_paths[idx]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image) # apply resize / normalization / convert to tensor \n",
    "        \n",
    "        # Tokenize Clinical Notes\n",
    "        text = str(self.notes[idx])\n",
    "        encoding = self.tokenizer(text, padding='max_length', truncation=True, max_length=self.max_len, return_tensors='pt')\n",
    "        input_ids = encoding['input_ids'].squeeze(0)  # (max_len,)\n",
    "        attention_mask = encoding['attention_mask'].squeeze(0)\n",
    "        \n",
    "        # Label\n",
    "        label = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        \n",
    "        return image, input_ids, attention_mask, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================\n",
    "#    Feature Extractors\n",
    "# ============================\n",
    "class FeatureExtractor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FeatureExtractor, self).__init__()\n",
    "        resnet = models.resnet18(pretrained=True)\n",
    "        self.cnn = nn.Sequential(*list(resnet.children())[:-1])  # Remove last FC layer\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "    \n",
    "    def forward(self, image, input_ids, attention_mask):\n",
    "        # Image Features\n",
    "        img_feat = self.cnn(image).flatten(1)  # (batch, 512)\n",
    "        \n",
    "        # Text Features\n",
    "        text_outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        text_feat = text_outputs.last_hidden_state[:, 0, :]  # CLS token (batch, 768)\n",
    "        \n",
    "        return img_feat, text_feat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class CAM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CAM, self).__init__()\n",
    "        self.encoder1 = nn.Linear(512, 128)  # Image\n",
    "        self.encoder2 = nn.Linear(768, 128)  # Text\n",
    "        self.affine_a = nn.Linear(128, 128, bias=False)\n",
    "        self.affine_v = nn.Linear(128, 128, bias=False)\n",
    "        self.W_ca = nn.Linear(128, 32, bias=False)  # Cross attention weight for image\n",
    "        self.W_cv = nn.Linear(128, 32, bias=False)  # Cross attention weight for text\n",
    "        self.W_ha = nn.Linear(32, 128, bias=False)  # Attention map weight for image\n",
    "        self.W_hv = nn.Linear(32, 128, bias=False)  # Attention map weight for text\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        # Fully connected layers for final prediction\n",
    "        self.regressor = nn.Sequential(\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 3),  # 3 classes: Normal, Benign, Malignant\n",
    "        )\n",
    "    \n",
    "    def forward(self, img_feat, text_feat):\n",
    "        # Image and text feature encoders\n",
    "        img_feat = self.encoder1(img_feat)  # X_v (image)\n",
    "        text_feat = self.encoder2(text_feat)  # X_a (text)\n",
    "\n",
    "        # Attention calculation for both modalities\n",
    "        att_img = self.affine_a(img_feat)\n",
    "        att_text = self.affine_v(text_feat)\n",
    "\n",
    "        # Joint attention for both modalities (image and text)\n",
    "        H_a = self.relu(self.W_ca(att_img))\n",
    "        H_v = self.relu(self.W_cv(att_text))\n",
    "\n",
    "        # Attending to features (modulation based on attention maps)\n",
    "        img_out = self.W_ha(H_a) + img_feat\n",
    "        text_out = self.W_hv(H_v) + text_feat\n",
    "\n",
    "        # Fusing the attended features (concatenation)\n",
    "        fused_feat = torch.cat((img_out, text_out), dim=1)\n",
    "\n",
    "        # Final regression to get prediction\n",
    "        output = self.regressor(fused_feat)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of image_paths:  503\n",
      "Length of notes:  503\n",
      "Length of labels:  503\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we already have the image paths, notes and labels\n",
    "image_paths = [i.replace('\\\\', '/') for i in image_paths]\n",
    "\n",
    "print(\"Length of image_paths: \", len(image_paths))\n",
    "print(\"Length of notes: \", len(notes))\n",
    "print(\"Length of labels: \", len(labels))\n",
    "\n",
    "# check for None values in labels\n",
    "labels.count(None)  # should be 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "BATCH_SIZE = 16\n",
    "MAX_LEN = 128\n",
    "EPOCHS = 10\n",
    "LEARNING_RATE = 1e-4\n",
    "\n",
    "# Data Transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Split the image_paths,notes and labels\n",
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "for train_index, val_index in sss.split(image_paths, labels):\n",
    "    train_image_paths, val_image_paths = [image_paths[i] for i in train_index], [image_paths[i] for i in val_index]\n",
    "    train_notes, val_notes = [notes[i] for i in train_index], [notes[i] for i in val_index]\n",
    "    train_labels, val_labels = [labels[i] for i in train_index], [labels[i] for i in val_index]\n",
    "\n",
    "\n",
    "# Create the training and validation datasets\n",
    "train_dataset = CancerDataset(train_image_paths, train_notes, train_labels, tokenizer, MAX_LEN, transform)\n",
    "val_dataset = CancerDataset(val_image_paths, val_notes, val_labels, tokenizer, MAX_LEN, transform)\n",
    "\n",
    "# DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|██████████| 26/26 [04:19<00:00,  9.99s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 1.0841, Training Accuracy: 0.4005, Training AUC-ROC: 0.5297\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Epoch 1/10: 100%|██████████| 7/7 [00:26<00:00,  3.72s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Validation Accuracy: 0.4158, Validation AUC-ROC: 0.6326\n",
      "Saved best model with Validation Accuracy: 0.4158 and Validation AUC-ROC: 0.6326\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|██████████| 26/26 [04:24<00:00, 10.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10, Loss: 1.0526, Training Accuracy: 0.4627, Training AUC-ROC: 0.6528\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Epoch 2/10: 100%|██████████| 7/7 [00:28<00:00,  4.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10, Validation Accuracy: 0.4158, Validation AUC-ROC: 0.6767\n",
      "Saved best model with Validation Accuracy: 0.4158 and Validation AUC-ROC: 0.6767\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 100%|██████████| 26/26 [04:23<00:00, 10.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10, Loss: 1.0452, Training Accuracy: 0.4975, Training AUC-ROC: 0.6729\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Epoch 3/10: 100%|██████████| 7/7 [00:25<00:00,  3.65s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10, Validation Accuracy: 0.4653, Validation AUC-ROC: 0.6885\n",
      "Saved best model with Validation Accuracy: 0.4653 and Validation AUC-ROC: 0.6885\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: 100%|██████████| 26/26 [04:23<00:00, 10.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10, Loss: 1.0077, Training Accuracy: 0.5448, Training AUC-ROC: 0.7287\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Epoch 4/10: 100%|██████████| 7/7 [00:25<00:00,  3.64s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10, Validation Accuracy: 0.4752, Validation AUC-ROC: 0.7060\n",
      "Saved best model with Validation Accuracy: 0.4752 and Validation AUC-ROC: 0.7060\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: 100%|██████████| 26/26 [04:31<00:00, 10.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10, Loss: 0.9962, Training Accuracy: 0.5423, Training AUC-ROC: 0.7494\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Epoch 5/10: 100%|██████████| 7/7 [00:27<00:00,  3.92s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10, Validation Accuracy: 0.5545, Validation AUC-ROC: 0.6819\n",
      "Saved best model with Validation Accuracy: 0.5545 and Validation AUC-ROC: 0.6819\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10: 100%|██████████| 26/26 [04:37<00:00, 10.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10, Loss: 0.9708, Training Accuracy: 0.5647, Training AUC-ROC: 0.7351\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Epoch 6/10: 100%|██████████| 7/7 [00:25<00:00,  3.65s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10, Validation Accuracy: 0.5149, Validation AUC-ROC: 0.7037\n",
      "Saved best model with Validation Accuracy: 0.5149 and Validation AUC-ROC: 0.7037\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10: 100%|██████████| 26/26 [04:29<00:00, 10.36s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10, Loss: 0.9356, Training Accuracy: 0.5771, Training AUC-ROC: 0.7787\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Epoch 7/10: 100%|██████████| 7/7 [00:24<00:00,  3.49s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10, Validation Accuracy: 0.5545, Validation AUC-ROC: 0.6942\n",
      "Saved best model with Validation Accuracy: 0.5545 and Validation AUC-ROC: 0.6942\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10: 100%|██████████| 26/26 [04:14<00:00,  9.77s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10, Loss: 0.9203, Training Accuracy: 0.5821, Training AUC-ROC: 0.7475\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Epoch 8/10: 100%|██████████| 7/7 [00:25<00:00,  3.67s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10, Validation Accuracy: 0.5743, Validation AUC-ROC: 0.7265\n",
      "Saved best model with Validation Accuracy: 0.5743 and Validation AUC-ROC: 0.7265\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10: 100%|██████████| 26/26 [04:09<00:00,  9.59s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10, Loss: 0.9042, Training Accuracy: 0.5746, Training AUC-ROC: 0.7802\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Epoch 9/10: 100%|██████████| 7/7 [00:25<00:00,  3.65s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10, Validation Accuracy: 0.5743, Validation AUC-ROC: 0.7106\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|██████████| 26/26 [04:21<00:00, 10.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10, Loss: 0.8847, Training Accuracy: 0.5821, Training AUC-ROC: 0.7801\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Epoch 10/10: 100%|██████████| 7/7 [00:25<00:00,  3.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10, Validation Accuracy: 0.5644, Validation AUC-ROC: 0.7259\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "extractor = FeatureExtractor().to(device)\n",
    "cam_model = CAM().to(device)\n",
    "\n",
    "# Use CrossEntropyLoss for multi-class classification\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Adam optimizer\n",
    "optimizer = optim.Adam(cam_model.parameters(), lr=LEARNING_RATE)\n",
    "# ============================\n",
    "\n",
    "\n",
    "#    Training Loop\n",
    "# ============================\n",
    "\n",
    "best_val_acc = 0.0  # Track the best validation accuracy\n",
    "best_val_auc = 0.0  # Track the best validation AUC score\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    cam_model.train()\n",
    "    extractor.train()\n",
    "    total_loss = 0\n",
    "    correct_preds = 0\n",
    "    total_preds = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    for images, input_ids, attention_mask, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS}\"):\n",
    "        images, input_ids, attention_mask, labels = images.to(device), input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Feature extraction\n",
    "        img_feat, text_feat = extractor(images, input_ids, attention_mask)\n",
    "        \n",
    "        # CAM model forward pass\n",
    "        outputs = cam_model(img_feat, text_feat)  # Multi-class outputs (logits)\n",
    "        \n",
    "        # Check if the output is a 2D tensor with shape [batch_size, num_classes]\n",
    "        assert outputs.shape[1] == 3, f\"Expected 3 classes in output, but got {outputs.shape[1]}\"\n",
    "        # Loss calculation (CrossEntropyLoss expects raw logits)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        _, predicted_labels = torch.max(outputs, 1)  # Get the class with the highest probability\n",
    "        correct_preds += (predicted_labels == labels).sum().item()\n",
    "        total_preds += labels.size(0)\n",
    "\n",
    "        # For AUC-ROC calculation, we need probabilities\n",
    "        probs = torch.nn.functional.softmax(outputs, dim=1)\n",
    "        all_preds.append(probs.cpu().detach().numpy())\n",
    "        all_labels.append(labels.cpu().detach().numpy())\n",
    "\n",
    "    train_acc = correct_preds / total_preds\n",
    "    all_preds = np.concatenate(all_preds, axis=0)\n",
    "    all_labels = np.concatenate(all_labels, axis=0)\n",
    "\n",
    "    # Compute AUC-ROC score\n",
    "    train_auc = roc_auc_score(all_labels, all_preds, multi_class='ovr', average='macro')\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS}, Loss: {total_loss/len(train_loader):.4f}, Training Accuracy: {train_acc:.4f}, Training AUC-ROC: {train_auc:.4f}\")\n",
    "    \n",
    "    # ============================\n",
    "    # Validation Loop\n",
    "    # ============================\n",
    "    cam_model.eval()  # Switch to evaluation mode\n",
    "    val_correct_preds = 0\n",
    "    val_total_preds = 0\n",
    "    val_all_preds = []\n",
    "    val_all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for images, input_ids, attention_mask, labels in tqdm(val_loader, desc=f\"Validation Epoch {epoch+1}/{EPOCHS}\"):\n",
    "            images, input_ids, attention_mask, labels = images.to(device), input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
    "            \n",
    "            # Feature extraction\n",
    "            img_feat, text_feat = extractor(images, input_ids, attention_mask)\n",
    "            \n",
    "            # CAM model forward pass\n",
    "            outputs = cam_model(img_feat, text_feat)  # Multi-class outputs (logits)\n",
    "            \n",
    "            # Calculate accuracy\n",
    "            _, predicted_labels = torch.max(outputs, 1)  # Get the class with the highest probability\n",
    "            val_correct_preds += (predicted_labels == labels).sum().item()\n",
    "            val_total_preds += labels.size(0)\n",
    "\n",
    "            # For AUC-ROC calculation, we need probabilities\n",
    "            probs = torch.nn.functional.softmax(outputs, dim=1)\n",
    "            val_all_preds.append(probs.cpu().detach().numpy())\n",
    "            val_all_labels.append(labels.cpu().detach().numpy())\n",
    "    \n",
    "    val_acc = val_correct_preds / val_total_preds\n",
    "    val_all_preds = np.concatenate(val_all_preds, axis=0)\n",
    "    val_all_labels = np.concatenate(val_all_labels, axis=0)\n",
    "\n",
    "    # Compute AUC-ROC score for validation\n",
    "    val_auc = roc_auc_score(val_all_labels, val_all_preds, multi_class='ovr', average='macro')\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS}, Validation Accuracy: {val_acc:.4f}, Validation AUC-ROC: {val_auc:.4f}\")\n",
    "    \n",
    "    # Save the best model based on validation accuracy or AUC\n",
    "    if val_acc > best_val_acc or val_auc > best_val_auc:\n",
    "        best_val_acc = val_acc\n",
    "        best_val_auc = val_auc\n",
    "        torch.save(cam_model.state_dict(), \"best_cancer_cam_model.pth\")\n",
    "        print(f\"Saved best model with Validation Accuracy: {val_acc:.4f} and Validation AUC-ROC: {val_auc:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
