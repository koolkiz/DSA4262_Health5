{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "data_path = '/content/drive/MyDrive/data'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tEDYYNqx-Cq1",
        "outputId": "45c33d00-dcfc-4483-8e83-1deb9b6485f8"
      },
      "execution_count": 264,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install python-docx"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wL9kFcsJE1Ie",
        "outputId": "a3a0674e-0f12-418f-b6bc-19bdec4530cb"
      },
      "execution_count": 265,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: python-docx in /usr/local/lib/python3.11/dist-packages (1.1.2)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from python-docx) (5.3.1)\n",
            "Requirement already satisfied: typing-extensions>=4.9.0 in /usr/local/lib/python3.11/dist-packages (from python-docx) (4.13.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 266,
      "metadata": {
        "id": "KL_fw9JM9LVB"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from transformers import BertTokenizer\n",
        "import os\n",
        "from docx import Document\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import models\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import f1_score, roc_auc_score, confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NXhqN1fb9LVP"
      },
      "source": [
        "# Data Prep"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 267,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "p_sQiNfy9LVT",
        "outputId": "07f8caea-d71a-45d6-cc29-06ecc1c21880"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n# redacted - not using clinical notes\\n\\n# Load and clean all clinical notes\\n#notes_path = \"data/notes\"\\nnotes_path = data_path + \\'/notes\\'\\ndef load_docx(file_path):\\n    \"\"\"Load a .docx file and extract text\"\"\"\\n    doc = Document(file_path)\\n    text = \"\\n\".join([para.text for para in doc.paragraphs])\\n    return text\\n\\ndef clean_clinical_text(text):\\n    \"\"\"Cleans clinical text for NLP processing.\"\"\"\\n    text = \"\\n\".join([line.strip() for line in text.split(\"\\n\") if line.strip()])\\n    text = text.lower()\\n    text = text.split(\"\\n\", 1)[1]\\n    text = text.replace(\"\\n\", \" \")\\n    return text\\n\\ndef load_clinical_notes(file_path):\\n    notes = {}\\n    for root, dirs, files in os.walk(file_path):\\n        for file in files:\\n            filename, file_extension = os.path.splitext(file)\\n            if file.endswith(\".docx\"):\\n                note = load_docx(os.path.join(root, file))\\n                note = clean_clinical_text(note)\\n                notes[filename] = note\\n    return notes\\n\\nd = load_clinical_notes(notes_path)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 267
        }
      ],
      "source": [
        "'''\n",
        "# redacted - not using clinical notes\n",
        "\n",
        "# Load and clean all clinical notes\n",
        "#notes_path = \"data/notes\"\n",
        "notes_path = data_path + '/notes'\n",
        "def load_docx(file_path):\n",
        "    \"\"\"Load a .docx file and extract text\"\"\"\n",
        "    doc = Document(file_path)\n",
        "    text = \"\\n\".join([para.text for para in doc.paragraphs])\n",
        "    return text\n",
        "\n",
        "def clean_clinical_text(text):\n",
        "    \"\"\"Cleans clinical text for NLP processing.\"\"\"\n",
        "    text = \"\\n\".join([line.strip() for line in text.split(\"\\n\") if line.strip()])\n",
        "    text = text.lower()\n",
        "    text = text.split(\"\\n\", 1)[1]\n",
        "    text = text.replace(\"\\n\", \" \")\n",
        "    return text\n",
        "\n",
        "def load_clinical_notes(file_path):\n",
        "    notes = {}\n",
        "    for root, dirs, files in os.walk(file_path):\n",
        "        for file in files:\n",
        "            filename, file_extension = os.path.splitext(file)\n",
        "            if file.endswith(\".docx\"):\n",
        "                note = load_docx(os.path.join(root, file))\n",
        "                note = clean_clinical_text(note)\n",
        "                notes[filename] = note\n",
        "    return notes\n",
        "\n",
        "d = load_clinical_notes(notes_path)\n",
        "'''\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 268,
      "metadata": {
        "id": "vaIw_apR9LVU"
      },
      "outputs": [],
      "source": [
        "#image_folder = 'data/images/all'\n",
        "#notes_folder = 'data/notes'\n",
        "image_folder = data_path + '/images/all'\n",
        "notes_folder = data_path + '/notes'\n",
        "\n",
        "image_list = os.listdir(image_folder)\n",
        "\n",
        "def extract_image_id(image_path):\n",
        "    # image_path = 'P100_L_CM_MLO.jpg'\n",
        "    # Extract the image ID from the filename\n",
        "\n",
        "    # Assuming the image ID is the part before the first underscore\n",
        "    image_id = image_path.split('_')[0]\n",
        "    return image_id\n",
        "\n",
        "image_ids = [extract_image_id(image_path) for image_path in image_list]\n",
        "\n",
        "# create list of paths for all 503 images and their corresponding notes\n",
        "image_paths = [os.path.join(image_folder, image_path) for image_path in image_list]\n",
        "notes = [d[image_id] if image_id in d else None for image_id in image_ids]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 269,
      "metadata": {
        "id": "DQSaJHuv9LVW"
      },
      "outputs": [],
      "source": [
        "# Now we have a list of image paths and their corresponding notes\n",
        "# Time to extract the corresponding labels from the CSV file\n",
        "\n",
        "# Length of image_paths, notes and labels = 503\n",
        "# Normal = 0\n",
        "# Benign = 1\n",
        "# Malignant = 2\n",
        "\n",
        "# Load the CSV file\n",
        "csv_path = data_path + '/clinical_data.csv'\n",
        "df = pd.read_csv(csv_path)\n",
        "labels = []\n",
        "notes = []\n",
        "encode = {'Benign': 1, 'Malignant': 2, 'Normal': 0}\n",
        "\n",
        "for id in image_list:\n",
        "    c = id.split('.')[0]\n",
        "\n",
        "    # extract row where column 'Image_name' = c and find pathology column\n",
        "    row = df[df['Image_name'] == c]\n",
        "    if not row.empty:\n",
        "        finding = row['Findings'].iloc[0]\n",
        "        notes.append(finding)\n",
        "\n",
        "        pathology = row['Pathology Classification/ Follow up'].values[0]\n",
        "        encode_pathology = encode.get(pathology, None)\n",
        "        if encode_pathology is not None:\n",
        "            labels.append(encode_pathology)\n",
        "        else:\n",
        "            # If the pathology is not in the encode dictionary, append None\n",
        "            print(f\"Pathology '{pathology}' not found in encoding dictionary for image {c}.\")\n",
        "    else:\n",
        "        print(c)\n",
        "        print(f\"No matching row found in CSV for image {c}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 270,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "64ipl9UL9LVX",
        "outputId": "db774c33-b46d-4edc-965f-9cf1751aa8db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of image_paths: 503\n",
            "Length of notes: 503\n",
            "Length of labels: 503\n",
            "Normal:  207\n",
            "Benign:  128\n",
            "Malignant:  168\n"
          ]
        }
      ],
      "source": [
        "# Check if the lengths of image_paths, notes, and labels match\n",
        "print(f\"Length of image_paths: {len(image_paths)}\")\n",
        "print(f\"Length of notes: {len(notes)}\")\n",
        "print(f\"Length of labels: {len(labels)}\")\n",
        "\n",
        "print(\"Normal: \", labels.count(0))\n",
        "print(\"Benign: \", labels.count(1))\n",
        "print(\"Malignant: \", labels.count(2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PEw5qisd9LVa"
      },
      "source": [
        "# Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 271,
      "metadata": {
        "id": "f5NeRkE_9LVf"
      },
      "outputs": [],
      "source": [
        "# ============================\n",
        "#    Custom Dataset Class\n",
        "# ============================\n",
        "class CancerDataset(Dataset):\n",
        "    def __init__(self, image_paths, notes, labels, tokenizer, max_len, transform=None):\n",
        "        self.image_paths = image_paths\n",
        "        self.notes = notes\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Load Image\n",
        "        img_path = self.image_paths[idx]\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "        if self.transform:\n",
        "            image = self.transform(image) # apply resize / normalization / convert to tensor\n",
        "\n",
        "        # Tokenize Clinical Notes\n",
        "        text = str(self.notes[idx])\n",
        "        encoding = self.tokenizer(text, padding='max_length', truncation=True, max_length=self.max_len, return_tensors='pt')\n",
        "        input_ids = encoding['input_ids'].squeeze(0)  # (max_len,)\n",
        "        attention_mask = encoding['attention_mask'].squeeze(0)\n",
        "\n",
        "        # Label\n",
        "        label = torch.tensor(self.labels[idx], dtype=torch.long)\n",
        "\n",
        "        return image, input_ids, attention_mask, label"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6yJvbnJW9LVg"
      },
      "source": [
        "# Feature Extraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 272,
      "metadata": {
        "id": "mQcC8N049LVi"
      },
      "outputs": [],
      "source": [
        "# ============================\n",
        "#    Feature Extractors\n",
        "# ============================\n",
        "class FeatureExtractor(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(FeatureExtractor, self).__init__()\n",
        "        resnet = models.resnet18(pretrained=True)\n",
        "        self.cnn = nn.Sequential(*list(resnet.children())[:-1])  # Remove last FC layer\n",
        "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "    def forward(self, image, input_ids, attention_mask):\n",
        "        # Image Features\n",
        "        img_feat = self.cnn(image).flatten(1)  # (batch, 512)\n",
        "\n",
        "        # Text Features\n",
        "        text_outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        text_feat = text_outputs.last_hidden_state[:, 0, :]  # CLS token (batch, 768)\n",
        "\n",
        "        return img_feat, text_feat"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2qOs-6bp9LVj"
      },
      "source": [
        "# Fusion"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "# self-attention\n",
        "\n",
        "class CAM(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CAM, self).__init__()\n",
        "        self.encoder1 = nn.Linear(512, 128)  # Image\n",
        "        self.encoder2 = nn.Linear(768, 128)  # Text\n",
        "        self.affine_a = nn.Linear(128, 128, bias=False)\n",
        "        self.affine_v = nn.Linear(128, 128, bias=False)\n",
        "        self.W_ca = nn.Linear(128, 32, bias=False)  # Cross attention weight for image\n",
        "        self.W_cv = nn.Linear(128, 32, bias=False)  # Cross attention weight for text\n",
        "        self.W_ha = nn.Linear(32, 128, bias=False)  # Attention map weight for image\n",
        "        self.W_hv = nn.Linear(32, 128, bias=False)  # Attention map weight for text\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "        # Fully connected layers for final prediction\n",
        "        self.regressor = nn.Sequential(\n",
        "            nn.Linear(256, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 3),  # 3 classes: Normal, Benign, Malignant\n",
        "        )\n",
        "\n",
        "    def forward(self, img_feat, text_feat):\n",
        "        # Image and text feature encoders\n",
        "        img_feat = self.encoder1(img_feat)  # X_v (image)\n",
        "        text_feat = self.encoder2(text_feat)  # X_a (text)\n",
        "\n",
        "        # Attention calculation for both modalities\n",
        "        att_img = self.affine_a(img_feat)\n",
        "        att_text = self.affine_v(text_feat)\n",
        "\n",
        "        # Joint attention for both modalities (image and text)\n",
        "        H_a = self.relu(self.W_ca(att_img))\n",
        "        H_v = self.relu(self.W_cv(att_text))\n",
        "\n",
        "        # Attending to features (modulation based on attention maps)\n",
        "        img_out = self.W_ha(H_a) + img_feat\n",
        "        text_out = self.W_hv(H_v) + text_feat\n",
        "\n",
        "        # Fusing the attended features (concatenation)\n",
        "        fused_feat = torch.cat((img_out, text_out), dim=1)\n",
        "\n",
        "        # Final regression to get prediction\n",
        "        output = self.regressor(fused_feat)\n",
        "        return output\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "iqFu2C6AWgSq",
        "outputId": "f1b85d5c-3b25-48c2-fff8-46ce2d42cb56"
      },
      "execution_count": 273,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n# self-attention\\n\\nclass CAM(nn.Module):\\n    def __init__(self):\\n        super(CAM, self).__init__()\\n        self.encoder1 = nn.Linear(512, 128)  # Image\\n        self.encoder2 = nn.Linear(768, 128)  # Text\\n        self.affine_a = nn.Linear(128, 128, bias=False)\\n        self.affine_v = nn.Linear(128, 128, bias=False)\\n        self.W_ca = nn.Linear(128, 32, bias=False)  # Cross attention weight for image\\n        self.W_cv = nn.Linear(128, 32, bias=False)  # Cross attention weight for text\\n        self.W_ha = nn.Linear(32, 128, bias=False)  # Attention map weight for image\\n        self.W_hv = nn.Linear(32, 128, bias=False)  # Attention map weight for text\\n        self.relu = nn.ReLU()\\n\\n        # Fully connected layers for final prediction\\n        self.regressor = nn.Sequential(\\n            nn.Linear(256, 128),\\n            nn.ReLU(),\\n            nn.Linear(128, 3),  # 3 classes: Normal, Benign, Malignant\\n        )\\n    \\n    def forward(self, img_feat, text_feat):\\n        # Image and text feature encoders\\n        img_feat = self.encoder1(img_feat)  # X_v (image)\\n        text_feat = self.encoder2(text_feat)  # X_a (text)\\n\\n        # Attention calculation for both modalities\\n        att_img = self.affine_a(img_feat)\\n        att_text = self.affine_v(text_feat)\\n\\n        # Joint attention for both modalities (image and text)\\n        H_a = self.relu(self.W_ca(att_img))\\n        H_v = self.relu(self.W_cv(att_text))\\n\\n        # Attending to features (modulation based on attention maps)\\n        img_out = self.W_ha(H_a) + img_feat\\n        text_out = self.W_hv(H_v) + text_feat\\n\\n        # Fusing the attended features (concatenation)\\n        fused_feat = torch.cat((img_out, text_out), dim=1)\\n\\n        # Final regression to get prediction\\n        output = self.regressor(fused_feat)\\n        return output\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 273
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 274,
      "metadata": {
        "id": "CxdHlkQg9LVj"
      },
      "outputs": [],
      "source": [
        "class CAM(nn.Module):\n",
        "    def __init__(self, embed_dim=128, num_heads=4):\n",
        "        super(CAM, self).__init__()\n",
        "\n",
        "        reduced_dim = embed_dim # continue same dim\n",
        "\n",
        "        # Enhanced projection layers with dropout\n",
        "        self.img_proj = nn.Sequential(\n",
        "            nn.Linear(512, embed_dim),\n",
        "            nn.Dropout(0.3)  # Added input dropout\n",
        "        )\n",
        "        self.text_proj = nn.Sequential(\n",
        "            nn.Linear(768, embed_dim),\n",
        "            nn.Dropout(0.3)\n",
        "        )\n",
        "\n",
        "        # Pre-normalization layers\n",
        "        self.norm1 = nn.LayerNorm(embed_dim)\n",
        "        self.norm2 = nn.LayerNorm(embed_dim)\n",
        "\n",
        "        # Cross-attention modules with increased dropout\n",
        "        self.img2text_attn = nn.MultiheadAttention(\n",
        "            embed_dim, num_heads, batch_first=True, dropout=0.3\n",
        "        )\n",
        "        self.text2img_attn = nn.MultiheadAttention(\n",
        "            embed_dim, num_heads, batch_first=True, dropout=0.3\n",
        "        )\n",
        "\n",
        "        # Feature processing with additional regularization\n",
        "        self.img_fc = nn.Sequential(\n",
        "            nn.Linear(embed_dim, reduced_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(0.3),  # Added feature dropout\n",
        "            nn.LayerNorm(reduced_dim)\n",
        "        )\n",
        "        self.text_fc = nn.Sequential(\n",
        "            nn.Linear(embed_dim, reduced_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.LayerNorm(reduced_dim)\n",
        "        )\n",
        "\n",
        "        # Simplified regressor with stronger dropout\n",
        "        self.regressor = nn.Sequential(\n",
        "            nn.Linear(2 * reduced_dim, 64),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(0.5),  # Increased dropout\n",
        "            nn.Linear(64, 3)\n",
        "        )\n",
        "\n",
        "    def forward(self, img_feat, text_feat):\n",
        "        # Project features with dropout\n",
        "        img_proj = self.img_proj(img_feat).unsqueeze(1)\n",
        "        text_proj = self.text_proj(text_feat).unsqueeze(1)\n",
        "\n",
        "        # Pre-normalization\n",
        "        img_norm = self.norm1(img_proj)\n",
        "        text_norm = self.norm2(text_proj)\n",
        "\n",
        "        # Cross-attention with residual connections\n",
        "        img_attn_out, _ = self.img2text_attn(\n",
        "            query=img_norm,\n",
        "            key=text_norm,\n",
        "            value=text_norm\n",
        "        )\n",
        "        img_attn_out = img_proj + img_attn_out  # Residual connection\n",
        "\n",
        "        text_attn_out, _ = self.text2img_attn(\n",
        "            query=text_norm,\n",
        "            key=img_norm,\n",
        "            value=img_norm\n",
        "        )\n",
        "        text_attn_out = text_proj + text_attn_out\n",
        "\n",
        "        # Process and fuse features\n",
        "        img_processed = self.img_fc(img_attn_out.squeeze(1))\n",
        "        text_processed = self.text_fc(text_attn_out.squeeze(1))\n",
        "\n",
        "        fused = torch.cat([img_processed, text_processed], dim=1)\n",
        "        return self.regressor(fused)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7h3dJHXX9LVl"
      },
      "source": [
        "# Training set up"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 275,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tNfJKAHu9LVl",
        "outputId": "54c5c68a-9b13-4c66-9c26-d73d4f102c6d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of image_paths:  503\n",
            "Length of notes:  503\n",
            "Length of labels:  503\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 275
        }
      ],
      "source": [
        "# we already have the image paths, notes and labels\n",
        "image_paths = [i.replace('\\\\', '/') for i in image_paths]\n",
        "\n",
        "print(\"Length of image_paths: \", len(image_paths))\n",
        "print(\"Length of notes: \", len(notes))\n",
        "print(\"Length of labels: \", len(labels))\n",
        "\n",
        "# check for None values in labels\n",
        "labels.count(None)  # should be 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 276,
      "metadata": {
        "id": "Srr7PSvD9LVm"
      },
      "outputs": [],
      "source": [
        "# Hyperparameters\n",
        "BATCH_SIZE = 16\n",
        "MAX_LEN = 128\n",
        "EPOCHS = 40\n",
        "LEARNING_RATE = 1e-4\n",
        "\n",
        "# Data Transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "'''\n",
        "# Split the image_paths,notes and labels\n",
        "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
        "for train_index, val_index in sss.split(image_paths, labels):\n",
        "    train_image_paths, val_image_paths = [image_paths[i] for i in train_index], [image_paths[i] for i in val_index]\n",
        "    train_notes, val_notes = [notes[i] for i in train_index], [notes[i] for i in val_index]\n",
        "    train_labels, val_labels = [labels[i] for i in train_index], [labels[i] for i in val_index]\n",
        "\n",
        "\n",
        "# Create the training and validation datasets\n",
        "train_dataset = CancerDataset(train_image_paths, train_notes, train_labels, tokenizer, MAX_LEN, transform)\n",
        "val_dataset = CancerDataset(val_image_paths, val_notes, val_labels, tokenizer, MAX_LEN, transform)\n",
        "\n",
        "# DataLoader\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "'''\n",
        "\n",
        "# to align with marcus train test split\n",
        "val_list = ['P251_R_CM_MLO', 'P55_R_CM_MLO', 'P261_L_CM_MLO', 'P96_R_CM_MLO', 'P111_R_CM_MLO', 'P281_R_CM_MLO', 'P278_L_CM_MLO', 'P280_L_CM_MLO', 'P110_L_CM_MLO', 'P59_R_CM_MLO', 'P155_L_CM_MLO', 'P318_R_CM_MLO', 'P131_L_CM_MLO', 'P123_R_CM_MLO', 'P1_L_CM_MLO', 'P322_L_CM_MLO', 'P271_L_CM_MLO', 'P56_L_CM_MLO', 'P246_R_CM_MLO', 'P16_R_CM_MLO', 'P214_L_CM_MLO', 'P213_L_CM_MLO', 'P230_R_CM_MLO', 'P288_L_CM_MLO', 'P312_L_CM_MLO', 'P193_L_CM_MLO', 'P139_R_CM_MLO', 'P46_L_CM_MLO', 'P179_L_CM_MLO', 'P148_R_CM_MLO', 'P62_L_CM_MLO', 'P210_L_CM_MLO', 'P40_R_CM_MLO', 'P138_L_CM_MLO', 'P311_L_CM_MLO', 'P71_L_CM_MLO', 'P104_L_CM_MLO', 'P295_L_CM_MLO', 'P295_R_CM_MLO', 'P161_L_CM_MLO', 'P2_L_CM_MLO', 'P80_L_CM_MLO', 'P228_L_CM_MLO', 'P319_R_CM_MLO', 'P319_L_CM_MLO', 'P128_L_CM_MLO', 'P182_R_CM_MLO', 'P271_R_CM_MLO', 'P61_L_CM_MLO', 'P21_R_CM_MLO', 'P236_L_CM_MLO', 'P196_L_CM_MLO', 'P11_R_CM_MLO', 'P324_R_CM_MLO', 'P191_R_CM_MLO', 'P67_R_CM_MLO', 'P270_R_CM_MLO', 'P307_R_CM_MLO', 'P181_R_CM_MLO', 'P8_L_CM_MLO', 'P54_L_CM_MLO', 'P14_L_CM_MLO', 'P65_R_CM_MLO', 'P77_L_CM_MLO', 'P310_R_CM_MLO', 'P302_R_CM_MLO', 'P52_R_CM_MLO', 'P52_L_CM_MLO', 'P226_R_CM_MLO', 'P273_R_CM_MLO', 'P226_L_CM_MLO', 'P24_L_CM_MLO', 'P29_R_CM_MLO', 'P175_R_CM_MLO', 'P58_L_CM_MLO', 'P127_L_CM_MLO', 'P59_L_CM_MLO', 'P268_R_CM_MLO', 'P282_L_CM_MLO', 'P92_R_CM_MLO', 'P123_L_CM_MLO', 'P219_L_CM_MLO', 'P7_R_CM_MLO', 'P146_L_CM_MLO', 'P291_L_CM_MLO', 'P322_R_CM_MLO', 'P207_R_CM_MLO', 'P190_R_CM_MLO', 'P197_L_CM_MLO', 'P215_R_CM_MLO', 'P325_L_CM_MLO', 'P33_L_CM_MLO', 'P276_R_CM_MLO', 'P51_L_CM_MLO', 'P324_L_CM_MLO', 'P206_R_CM_MLO', 'P56_R_CM_MLO', 'P237_R_CM_MLO', 'P312_R_CM_MLO', 'P158_L_CM_MLO', 'P230_L_CM_MLO']\n",
        "\n",
        "train_indexes = []\n",
        "val_indexes = []\n",
        "\n",
        "for i in range(len(image_paths)):\n",
        "  path = image_paths[i]\n",
        "  id = path.split('/')[-1].split('.')[0]\n",
        "  if id in val_list:\n",
        "    val_indexes.append(i)\n",
        "  else:\n",
        "    train_indexes.append(i)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Split data based on indexes\n",
        "train_image_paths = [image_paths[i] for i in train_indexes]\n",
        "train_notes = [notes[i] for i in train_indexes]\n",
        "train_labels = [labels[i] for i in train_indexes]\n",
        "\n",
        "val_image_paths = [image_paths[i] for i in val_indexes]\n",
        "val_notes = [notes[i] for i in val_indexes]\n",
        "val_labels = [labels[i] for i in val_indexes]\n",
        "\n",
        "# Create Datasets\n",
        "train_dataset = CancerDataset(train_image_paths, train_notes, train_labels, tokenizer, MAX_LEN, transform)\n",
        "val_dataset = CancerDataset(val_image_paths, val_notes, val_labels, tokenizer, MAX_LEN, transform)\n",
        "\n",
        "# Create Dataloaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n"
      ],
      "metadata": {
        "id": "V3rv4fbhU66c"
      },
      "execution_count": 277,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZRFtK1p19LVn"
      },
      "source": [
        "# Train and Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 278,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "d-N2rUc39LVn",
        "outputId": "0e624432-3527-45af-f031-3cdf4fd2a278"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\ndevice = torch.device(\\'cuda\\' if torch.cuda.is_available() else \\'cpu\\')\\nextractor = FeatureExtractor().to(device)\\ncam_model = CAM().to(device)\\n\\n# Use CrossEntropyLoss for multi-class classification\\ncriterion = nn.CrossEntropyLoss()\\n\\n# Adam optimizer\\noptimizer = optim.Adam(cam_model.parameters(), lr=LEARNING_RATE)\\n# ============================\\n\\n\\n#    Training Loop\\n# ============================\\n\\nbest_val_acc = 0.0  # Track the best validation accuracy\\nbest_val_auc = 0.0  # Track the best validation AUC score\\n\\nfor epoch in range(EPOCHS):\\n    cam_model.train()\\n    extractor.train()\\n    total_loss = 0\\n    correct_preds = 0\\n    total_preds = 0\\n    all_preds = []\\n    all_labels = []\\n    \\n    for images, input_ids, attention_mask, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS}\"):\\n        images, input_ids, attention_mask, labels = images.to(device), input_ids.to(device), attention_mask.to(device), labels.to(device)\\n        \\n        optimizer.zero_grad()\\n        \\n        # Feature extraction\\n        img_feat, text_feat = extractor(images, input_ids, attention_mask)\\n        \\n        # CAM model forward pass\\n        outputs = cam_model(img_feat, text_feat)  # Multi-class outputs (logits)\\n        \\n        # Check if the output is a 2D tensor with shape [batch_size, num_classes]\\n        assert outputs.shape[1] == 3, f\"Expected 3 classes in output, but got {outputs.shape[1]}\"\\n        # Loss calculation (CrossEntropyLoss expects raw logits)\\n        loss = criterion(outputs, labels)\\n        loss.backward()\\n        optimizer.step()\\n        \\n        total_loss += loss.item()\\n        \\n        # Calculate accuracy\\n        _, predicted_labels = torch.max(outputs, 1)  # Get the class with the highest probability\\n        correct_preds += (predicted_labels == labels).sum().item()\\n        total_preds += labels.size(0)\\n\\n        # For AUC-ROC calculation, we need probabilities\\n        probs = torch.nn.functional.softmax(outputs, dim=1)\\n        all_preds.append(probs.cpu().detach().numpy())\\n        all_labels.append(labels.cpu().detach().numpy())\\n\\n    train_acc = correct_preds / total_preds\\n    all_preds = np.concatenate(all_preds, axis=0)\\n    all_labels = np.concatenate(all_labels, axis=0)\\n\\n    # Compute AUC-ROC score\\n    train_auc = roc_auc_score(all_labels, all_preds, multi_class=\\'ovr\\', average=\\'macro\\')\\n    \\n    print(f\"Epoch {epoch+1}/{EPOCHS}, Loss: {total_loss/len(train_loader):.4f}, Training Accuracy: {train_acc:.4f}, Training AUC-ROC: {train_auc:.4f}\")\\n    \\n    # ============================\\n    # Validation Loop\\n    # ============================\\n    cam_model.eval()  # Switch to evaluation mode\\n    val_correct_preds = 0\\n    val_total_preds = 0\\n    val_all_preds = []\\n    val_all_labels = []\\n    with torch.no_grad():\\n        for images, input_ids, attention_mask, labels in tqdm(val_loader, desc=f\"Validation Epoch {epoch+1}/{EPOCHS}\"):\\n            images, input_ids, attention_mask, labels = images.to(device), input_ids.to(device), attention_mask.to(device), labels.to(device)\\n            \\n            # Feature extraction\\n            img_feat, text_feat = extractor(images, input_ids, attention_mask)\\n            \\n            # CAM model forward pass\\n            outputs = cam_model(img_feat, text_feat)  # Multi-class outputs (logits)\\n            \\n            # Calculate accuracy\\n            _, predicted_labels = torch.max(outputs, 1)  # Get the class with the highest probability\\n            val_correct_preds += (predicted_labels == labels).sum().item()\\n            val_total_preds += labels.size(0)\\n\\n            # For AUC-ROC calculation, we need probabilities\\n            probs = torch.nn.functional.softmax(outputs, dim=1)\\n            val_all_preds.append(probs.cpu().detach().numpy())\\n            val_all_labels.append(labels.cpu().detach().numpy())\\n    \\n    val_acc = val_correct_preds / val_total_preds\\n    val_all_preds = np.concatenate(val_all_preds, axis=0)\\n    val_all_labels = np.concatenate(val_all_labels, axis=0)\\n\\n    # Compute AUC-ROC score for validation\\n    val_auc = roc_auc_score(val_all_labels, val_all_preds, multi_class=\\'ovr\\', average=\\'macro\\')\\n    \\n    print(f\"Epoch {epoch+1}/{EPOCHS}, Validation Accuracy: {val_acc:.4f}, Validation AUC-ROC: {val_auc:.4f}\")\\n    \\n    # Save the best model based on validation accuracy or AUC\\n    if val_acc > best_val_acc or val_auc > best_val_auc:\\n        best_val_acc = val_acc\\n        best_val_auc = val_auc\\n        torch.save(cam_model.state_dict(), \"best_cancer_cam_model.pth\")\\n        print(f\"Saved best model with Validation Accuracy: {val_acc:.4f} and Validation AUC-ROC: {val_auc:.4f}\")\\n    '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 278
        }
      ],
      "source": [
        "'''\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "extractor = FeatureExtractor().to(device)\n",
        "cam_model = CAM().to(device)\n",
        "\n",
        "# Use CrossEntropyLoss for multi-class classification\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Adam optimizer\n",
        "optimizer = optim.Adam(cam_model.parameters(), lr=LEARNING_RATE)\n",
        "# ============================\n",
        "\n",
        "\n",
        "#    Training Loop\n",
        "# ============================\n",
        "\n",
        "best_val_acc = 0.0  # Track the best validation accuracy\n",
        "best_val_auc = 0.0  # Track the best validation AUC score\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    cam_model.train()\n",
        "    extractor.train()\n",
        "    total_loss = 0\n",
        "    correct_preds = 0\n",
        "    total_preds = 0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    for images, input_ids, attention_mask, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS}\"):\n",
        "        images, input_ids, attention_mask, labels = images.to(device), input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Feature extraction\n",
        "        img_feat, text_feat = extractor(images, input_ids, attention_mask)\n",
        "\n",
        "        # CAM model forward pass\n",
        "        outputs = cam_model(img_feat, text_feat)  # Multi-class outputs (logits)\n",
        "\n",
        "        # Check if the output is a 2D tensor with shape [batch_size, num_classes]\n",
        "        assert outputs.shape[1] == 3, f\"Expected 3 classes in output, but got {outputs.shape[1]}\"\n",
        "        # Loss calculation (CrossEntropyLoss expects raw logits)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Calculate accuracy\n",
        "        _, predicted_labels = torch.max(outputs, 1)  # Get the class with the highest probability\n",
        "        correct_preds += (predicted_labels == labels).sum().item()\n",
        "        total_preds += labels.size(0)\n",
        "\n",
        "        # For AUC-ROC calculation, we need probabilities\n",
        "        probs = torch.nn.functional.softmax(outputs, dim=1)\n",
        "        all_preds.append(probs.cpu().detach().numpy())\n",
        "        all_labels.append(labels.cpu().detach().numpy())\n",
        "\n",
        "    train_acc = correct_preds / total_preds\n",
        "    all_preds = np.concatenate(all_preds, axis=0)\n",
        "    all_labels = np.concatenate(all_labels, axis=0)\n",
        "\n",
        "    # Compute AUC-ROC score\n",
        "    train_auc = roc_auc_score(all_labels, all_preds, multi_class='ovr', average='macro')\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{EPOCHS}, Loss: {total_loss/len(train_loader):.4f}, Training Accuracy: {train_acc:.4f}, Training AUC-ROC: {train_auc:.4f}\")\n",
        "\n",
        "    # ============================\n",
        "    # Validation Loop\n",
        "    # ============================\n",
        "    cam_model.eval()  # Switch to evaluation mode\n",
        "    val_correct_preds = 0\n",
        "    val_total_preds = 0\n",
        "    val_all_preds = []\n",
        "    val_all_labels = []\n",
        "    with torch.no_grad():\n",
        "        for images, input_ids, attention_mask, labels in tqdm(val_loader, desc=f\"Validation Epoch {epoch+1}/{EPOCHS}\"):\n",
        "            images, input_ids, attention_mask, labels = images.to(device), input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
        "\n",
        "            # Feature extraction\n",
        "            img_feat, text_feat = extractor(images, input_ids, attention_mask)\n",
        "\n",
        "            # CAM model forward pass\n",
        "            outputs = cam_model(img_feat, text_feat)  # Multi-class outputs (logits)\n",
        "\n",
        "            # Calculate accuracy\n",
        "            _, predicted_labels = torch.max(outputs, 1)  # Get the class with the highest probability\n",
        "            val_correct_preds += (predicted_labels == labels).sum().item()\n",
        "            val_total_preds += labels.size(0)\n",
        "\n",
        "            # For AUC-ROC calculation, we need probabilities\n",
        "            probs = torch.nn.functional.softmax(outputs, dim=1)\n",
        "            val_all_preds.append(probs.cpu().detach().numpy())\n",
        "            val_all_labels.append(labels.cpu().detach().numpy())\n",
        "\n",
        "    val_acc = val_correct_preds / val_total_preds\n",
        "    val_all_preds = np.concatenate(val_all_preds, axis=0)\n",
        "    val_all_labels = np.concatenate(val_all_labels, axis=0)\n",
        "\n",
        "    # Compute AUC-ROC score for validation\n",
        "    val_auc = roc_auc_score(val_all_labels, val_all_preds, multi_class='ovr', average='macro')\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{EPOCHS}, Validation Accuracy: {val_acc:.4f}, Validation AUC-ROC: {val_auc:.4f}\")\n",
        "\n",
        "    # Save the best model based on validation accuracy or AUC\n",
        "    if val_acc > best_val_acc or val_auc > best_val_auc:\n",
        "        best_val_acc = val_acc\n",
        "        best_val_auc = val_auc\n",
        "        torch.save(cam_model.state_dict(), \"best_cancer_cam_model.pth\")\n",
        "        print(f\"Saved best model with Validation Accuracy: {val_acc:.4f} and Validation AUC-ROC: {val_auc:.4f}\")\n",
        "    '''\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "extractor = FeatureExtractor().to(device)\n",
        "cam_model = CAM().to(device)\n",
        "\n",
        "# Use CrossEntropyLoss for multi-class classification\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Adam optimizer\n",
        "optimizer = optim.Adam(cam_model.parameters(),\n",
        "                       lr=LEARNING_RATE,\n",
        "                       weight_decay=1e-2) # L2\n",
        "\n",
        "best_val_acc = 0.0\n",
        "best_val_auc = 0.0\n",
        "best_val_f1 = 0.0\n",
        "best_conf_matrix = None\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    cam_model.train()\n",
        "    extractor.train()\n",
        "    total_loss = 0\n",
        "    correct_preds = 0\n",
        "    total_preds = 0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    for images, input_ids, attention_mask, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS}\"):\n",
        "        images, input_ids, attention_mask, labels = images.to(device), input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Feature extraction\n",
        "        img_feat, text_feat = extractor(images, input_ids, attention_mask)\n",
        "\n",
        "        # CAM model forward pass\n",
        "        outputs = cam_model(img_feat, text_feat)  # Multi-class outputs (logits)\n",
        "\n",
        "        assert outputs.shape[1] == 3, f\"Expected 3 classes in output, but got {outputs.shape[1]}\"\n",
        "\n",
        "        # Loss calculation\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Compute predictions\n",
        "        _, predicted_labels = torch.max(outputs, 1)\n",
        "        correct_preds += (predicted_labels == labels).sum().item()\n",
        "        total_preds += labels.size(0)\n",
        "\n",
        "        all_preds.append(torch.nn.functional.softmax(outputs, dim=1).cpu().detach().numpy())  # Probabilities\n",
        "        all_labels.append(labels.cpu().detach().numpy())\n",
        "\n",
        "    train_acc = correct_preds / total_preds\n",
        "    all_preds = np.concatenate(all_preds, axis=0)\n",
        "    all_labels = np.concatenate(all_labels, axis=0)\n",
        "\n",
        "    train_f1 = f1_score(all_labels, np.argmax(all_preds, axis=1), average='macro')\n",
        "    train_auc = roc_auc_score(all_labels, all_preds, multi_class='ovr', average='macro')\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{EPOCHS}, Loss: {total_loss/len(train_loader):.4f}, Training Accuracy: {train_acc:.4f}, Training F1-score: {train_f1:.4f}, Training AUC-ROC: {train_auc:.4f}\")\n",
        "\n",
        "    # ============================\n",
        "    # Validation Loop\n",
        "    # ============================\n",
        "    cam_model.eval()\n",
        "    val_correct_preds = 0\n",
        "    val_total_preds = 0\n",
        "    val_all_preds = []\n",
        "    val_all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, input_ids, attention_mask, labels in tqdm(val_loader, desc=f\"Validation Epoch {epoch+1}/{EPOCHS}\"):\n",
        "            images, input_ids, attention_mask, labels = images.to(device), input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
        "\n",
        "            # Feature extraction\n",
        "            img_feat, text_feat = extractor(images, input_ids, attention_mask)\n",
        "\n",
        "            # CAM model forward pass\n",
        "            outputs = cam_model(img_feat, text_feat)  # Multi-class outputs (logits)\n",
        "\n",
        "            _, predicted_labels = torch.max(outputs, 1)\n",
        "            val_correct_preds += (predicted_labels == labels).sum().item()\n",
        "            val_total_preds += labels.size(0)\n",
        "\n",
        "            val_all_preds.append(torch.nn.functional.softmax(outputs, dim=1).cpu().detach().numpy())  # Probabilities\n",
        "            val_all_labels.append(labels.cpu().detach().numpy())\n",
        "\n",
        "    val_acc = val_correct_preds / val_total_preds\n",
        "    val_all_preds = np.concatenate(val_all_preds, axis=0)\n",
        "    val_all_labels = np.concatenate(val_all_labels, axis=0)\n",
        "\n",
        "    val_f1 = f1_score(val_all_labels, np.argmax(val_all_preds, axis=1), average='macro')\n",
        "    val_auc = roc_auc_score(val_all_labels, val_all_preds, multi_class='ovr', average='macro')\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{EPOCHS}, Validation Accuracy: {val_acc:.4f}, Validation F1-score: {val_f1:.4f}, Validation AUC-ROC: {val_auc:.4f}\")\n",
        "\n",
        "    # Save the best model based on validation accuracy\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        best_val_auc = val_auc\n",
        "        best_val_f1 = val_f1\n",
        "        val_predictions = np.argmax(val_all_preds, axis=1)\n",
        "        best_conf_matrix = confusion_matrix(val_all_labels, val_predictions)\n",
        "        torch.save(cam_model.state_dict(), \"best_cancer_cam_model.pth\")\n",
        "\n",
        "\n",
        "# ============================\n",
        "# Print Final Best Statistics\n",
        "# ============================\n",
        "print(\"\\nBest Model Statistics:\")\n",
        "print(f\"Best Validation Accuracy: {best_val_acc:.4f}\")\n",
        "print(f\"Best Validation F1-score: {best_val_f1:.4f}\")\n",
        "print(f\"Best Validation AUC-ROC: {best_val_auc:.4f}\")\n",
        "\n",
        "# Plot the confusion matrix\n",
        "plt.figure(figsize=(6, 6))\n",
        "sns.heatmap(best_conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=[0,1,2], yticklabels=[0,1,2])\n",
        "plt.xlabel(\"Predicted Labels\")\n",
        "plt.ylabel(\"True Labels\")\n",
        "plt.title(\"Confusion Matrix for Best Model\")\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Pp8tyLhjlVQ9",
        "outputId": "00238ad4-2978-4637-d0c8-177c65cf1e5c"
      },
      "execution_count": 279,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Epoch 1/40: 100%|██████████| 26/26 [00:46<00:00,  1.79s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/40, Loss: 1.0495, Training Accuracy: 0.4826, Training F1-score: 0.3679, Training AUC-ROC: 0.5874\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation Epoch 1/40: 100%|██████████| 7/7 [00:05<00:00,  1.35it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/40, Validation Accuracy: 0.6238, Validation F1-score: 0.4902, Validation AUC-ROC: 0.8138\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/40: 100%|██████████| 26/26 [00:28<00:00,  1.10s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/40, Loss: 1.0038, Training Accuracy: 0.5697, Training F1-score: 0.4715, Training AUC-ROC: 0.6729\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation Epoch 2/40: 100%|██████████| 7/7 [00:06<00:00,  1.11it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/40, Validation Accuracy: 0.6139, Validation F1-score: 0.4799, Validation AUC-ROC: 0.8831\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/40: 100%|██████████| 26/26 [00:29<00:00,  1.12s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/40, Loss: 0.8870, Training Accuracy: 0.6891, Training F1-score: 0.5877, Training AUC-ROC: 0.8143\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation Epoch 3/40: 100%|██████████| 7/7 [00:05<00:00,  1.36it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/40, Validation Accuracy: 0.6733, Validation F1-score: 0.5690, Validation AUC-ROC: 0.9162\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/40: 100%|██████████| 26/26 [00:30<00:00,  1.19s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/40, Loss: 0.7248, Training Accuracy: 0.7711, Training F1-score: 0.6922, Training AUC-ROC: 0.8878\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation Epoch 4/40: 100%|██████████| 7/7 [00:05<00:00,  1.37it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/40, Validation Accuracy: 0.7030, Validation F1-score: 0.6132, Validation AUC-ROC: 0.8956\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/40: 100%|██████████| 26/26 [00:28<00:00,  1.09s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/40, Loss: 0.6063, Training Accuracy: 0.8085, Training F1-score: 0.7474, Training AUC-ROC: 0.9046\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation Epoch 5/40: 100%|██████████| 7/7 [00:06<00:00,  1.12it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/40, Validation Accuracy: 0.8020, Validation F1-score: 0.7664, Validation AUC-ROC: 0.9284\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6/40: 100%|██████████| 26/26 [00:28<00:00,  1.08s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6/40, Loss: 0.4820, Training Accuracy: 0.8383, Training F1-score: 0.7845, Training AUC-ROC: 0.9384\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation Epoch 6/40: 100%|██████████| 7/7 [00:05<00:00,  1.36it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6/40, Validation Accuracy: 0.8020, Validation F1-score: 0.7667, Validation AUC-ROC: 0.9187\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7/40: 100%|██████████| 26/26 [00:28<00:00,  1.09s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7/40, Loss: 0.4459, Training Accuracy: 0.8781, Training F1-score: 0.8544, Training AUC-ROC: 0.9502\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation Epoch 7/40: 100%|██████████| 7/7 [00:06<00:00,  1.12it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7/40, Validation Accuracy: 0.7921, Validation F1-score: 0.7530, Validation AUC-ROC: 0.9139\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8/40: 100%|██████████| 26/26 [00:28<00:00,  1.10s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8/40, Loss: 0.3994, Training Accuracy: 0.8756, Training F1-score: 0.8515, Training AUC-ROC: 0.9519\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation Epoch 8/40: 100%|██████████| 7/7 [00:05<00:00,  1.29it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8/40, Validation Accuracy: 0.8020, Validation F1-score: 0.7664, Validation AUC-ROC: 0.9210\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9/40: 100%|██████████| 26/26 [00:29<00:00,  1.13s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9/40, Loss: 0.3812, Training Accuracy: 0.8831, Training F1-score: 0.8575, Training AUC-ROC: 0.9478\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation Epoch 9/40: 100%|██████████| 7/7 [00:05<00:00,  1.34it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9/40, Validation Accuracy: 0.8119, Validation F1-score: 0.7796, Validation AUC-ROC: 0.9319\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10/40: 100%|██████████| 26/26 [00:29<00:00,  1.14s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10/40, Loss: 0.3327, Training Accuracy: 0.8806, Training F1-score: 0.8522, Training AUC-ROC: 0.9591\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation Epoch 10/40: 100%|██████████| 7/7 [00:06<00:00,  1.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10/40, Validation Accuracy: 0.8119, Validation F1-score: 0.7822, Validation AUC-ROC: 0.9255\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 11/40: 100%|██████████| 26/26 [00:28<00:00,  1.11s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 11/40, Loss: 0.3624, Training Accuracy: 0.8731, Training F1-score: 0.8496, Training AUC-ROC: 0.9541\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation Epoch 11/40: 100%|██████████| 7/7 [00:05<00:00,  1.34it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 11/40, Validation Accuracy: 0.8119, Validation F1-score: 0.7812, Validation AUC-ROC: 0.9403\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 12/40: 100%|██████████| 26/26 [00:29<00:00,  1.12s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 12/40, Loss: 0.3430, Training Accuracy: 0.8881, Training F1-score: 0.8680, Training AUC-ROC: 0.9534\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation Epoch 12/40: 100%|██████████| 7/7 [00:05<00:00,  1.21it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 12/40, Validation Accuracy: 0.8218, Validation F1-score: 0.7925, Validation AUC-ROC: 0.9239\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 13/40: 100%|██████████| 26/26 [00:28<00:00,  1.11s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 13/40, Loss: 0.3792, Training Accuracy: 0.8881, Training F1-score: 0.8644, Training AUC-ROC: 0.9560\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation Epoch 13/40: 100%|██████████| 7/7 [00:06<00:00,  1.17it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 13/40, Validation Accuracy: 0.8119, Validation F1-score: 0.7817, Validation AUC-ROC: 0.9375\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 14/40: 100%|██████████| 26/26 [00:28<00:00,  1.10s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 14/40, Loss: 0.3401, Training Accuracy: 0.9030, Training F1-score: 0.8826, Training AUC-ROC: 0.9483\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation Epoch 14/40: 100%|██████████| 7/7 [00:05<00:00,  1.35it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 14/40, Validation Accuracy: 0.8317, Validation F1-score: 0.8052, Validation AUC-ROC: 0.9322\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 15/40: 100%|██████████| 26/26 [00:28<00:00,  1.09s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 15/40, Loss: 0.3123, Training Accuracy: 0.8980, Training F1-score: 0.8777, Training AUC-ROC: 0.9608\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation Epoch 15/40: 100%|██████████| 7/7 [00:06<00:00,  1.13it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 15/40, Validation Accuracy: 0.8218, Validation F1-score: 0.7937, Validation AUC-ROC: 0.9292\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 16/40: 100%|██████████| 26/26 [00:28<00:00,  1.08s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 16/40, Loss: 0.2897, Training Accuracy: 0.9055, Training F1-score: 0.8875, Training AUC-ROC: 0.9637\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation Epoch 16/40: 100%|██████████| 7/7 [00:05<00:00,  1.36it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 16/40, Validation Accuracy: 0.8317, Validation F1-score: 0.8052, Validation AUC-ROC: 0.9185\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 17/40: 100%|██████████| 26/26 [00:28<00:00,  1.10s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 17/40, Loss: 0.2787, Training Accuracy: 0.8955, Training F1-score: 0.8750, Training AUC-ROC: 0.9684\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation Epoch 17/40: 100%|██████████| 7/7 [00:05<00:00,  1.18it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 17/40, Validation Accuracy: 0.8416, Validation F1-score: 0.8177, Validation AUC-ROC: 0.9299\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 18/40: 100%|██████████| 26/26 [00:29<00:00,  1.15s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 18/40, Loss: 0.2659, Training Accuracy: 0.9204, Training F1-score: 0.9060, Training AUC-ROC: 0.9719\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation Epoch 18/40: 100%|██████████| 7/7 [00:06<00:00,  1.16it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 18/40, Validation Accuracy: 0.8218, Validation F1-score: 0.7937, Validation AUC-ROC: 0.9343\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 19/40: 100%|██████████| 26/26 [00:28<00:00,  1.10s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 19/40, Loss: 0.2584, Training Accuracy: 0.9154, Training F1-score: 0.8999, Training AUC-ROC: 0.9734\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation Epoch 19/40: 100%|██████████| 7/7 [00:05<00:00,  1.36it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 19/40, Validation Accuracy: 0.8416, Validation F1-score: 0.8183, Validation AUC-ROC: 0.9241\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 20/40: 100%|██████████| 26/26 [00:28<00:00,  1.08s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 20/40, Loss: 0.2797, Training Accuracy: 0.9080, Training F1-score: 0.8903, Training AUC-ROC: 0.9665\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation Epoch 20/40: 100%|██████████| 7/7 [00:06<00:00,  1.12it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 20/40, Validation Accuracy: 0.8614, Validation F1-score: 0.8421, Validation AUC-ROC: 0.9315\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 21/40: 100%|██████████| 26/26 [00:28<00:00,  1.09s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 21/40, Loss: 0.2611, Training Accuracy: 0.9129, Training F1-score: 0.8982, Training AUC-ROC: 0.9722\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation Epoch 21/40: 100%|██████████| 7/7 [00:05<00:00,  1.35it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 21/40, Validation Accuracy: 0.8416, Validation F1-score: 0.8177, Validation AUC-ROC: 0.9426\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 22/40: 100%|██████████| 26/26 [00:28<00:00,  1.11s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 22/40, Loss: 0.2438, Training Accuracy: 0.9129, Training F1-score: 0.8986, Training AUC-ROC: 0.9764\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation Epoch 22/40: 100%|██████████| 7/7 [00:05<00:00,  1.18it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 22/40, Validation Accuracy: 0.8218, Validation F1-score: 0.7937, Validation AUC-ROC: 0.9364\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 23/40: 100%|██████████| 26/26 [00:28<00:00,  1.09s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 23/40, Loss: 0.2550, Training Accuracy: 0.9104, Training F1-score: 0.8930, Training AUC-ROC: 0.9764\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation Epoch 23/40: 100%|██████████| 7/7 [00:05<00:00,  1.26it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 23/40, Validation Accuracy: 0.8515, Validation F1-score: 0.8300, Validation AUC-ROC: 0.9292\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 24/40: 100%|██████████| 26/26 [00:29<00:00,  1.12s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 24/40, Loss: 0.2818, Training Accuracy: 0.9154, Training F1-score: 0.9023, Training AUC-ROC: 0.9652\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation Epoch 24/40: 100%|██████████| 7/7 [00:05<00:00,  1.35it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 24/40, Validation Accuracy: 0.8416, Validation F1-score: 0.8183, Validation AUC-ROC: 0.9412\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 25/40: 100%|██████████| 26/26 [00:28<00:00,  1.09s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 25/40, Loss: 0.2584, Training Accuracy: 0.9055, Training F1-score: 0.8898, Training AUC-ROC: 0.9745\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation Epoch 25/40: 100%|██████████| 7/7 [00:06<00:00,  1.13it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 25/40, Validation Accuracy: 0.8515, Validation F1-score: 0.8304, Validation AUC-ROC: 0.9326\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 26/40: 100%|██████████| 26/26 [00:29<00:00,  1.14s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 26/40, Loss: 0.2503, Training Accuracy: 0.9129, Training F1-score: 0.8978, Training AUC-ROC: 0.9752\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation Epoch 26/40: 100%|██████████| 7/7 [00:05<00:00,  1.35it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 26/40, Validation Accuracy: 0.7822, Validation F1-score: 0.7445, Validation AUC-ROC: 0.9125\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 27/40: 100%|██████████| 26/26 [00:28<00:00,  1.11s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 27/40, Loss: 0.2262, Training Accuracy: 0.9204, Training F1-score: 0.9070, Training AUC-ROC: 0.9799\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation Epoch 27/40: 100%|██████████| 7/7 [00:05<00:00,  1.21it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 27/40, Validation Accuracy: 0.8515, Validation F1-score: 0.8304, Validation AUC-ROC: 0.9314\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 28/40: 100%|██████████| 26/26 [00:28<00:00,  1.10s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 28/40, Loss: 0.2294, Training Accuracy: 0.9204, Training F1-score: 0.9059, Training AUC-ROC: 0.9765\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation Epoch 28/40: 100%|██████████| 7/7 [00:05<00:00,  1.21it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 28/40, Validation Accuracy: 0.8317, Validation F1-score: 0.8066, Validation AUC-ROC: 0.9209\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 29/40: 100%|██████████| 26/26 [00:28<00:00,  1.11s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 29/40, Loss: 0.2207, Training Accuracy: 0.9204, Training F1-score: 0.9066, Training AUC-ROC: 0.9792\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation Epoch 29/40: 100%|██████████| 7/7 [00:05<00:00,  1.33it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 29/40, Validation Accuracy: 0.8416, Validation F1-score: 0.8183, Validation AUC-ROC: 0.9268\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 30/40: 100%|██████████| 26/26 [00:28<00:00,  1.10s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 30/40, Loss: 0.2057, Training Accuracy: 0.9328, Training F1-score: 0.9218, Training AUC-ROC: 0.9805\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation Epoch 30/40: 100%|██████████| 7/7 [00:06<00:00,  1.12it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 30/40, Validation Accuracy: 0.8119, Validation F1-score: 0.7812, Validation AUC-ROC: 0.9317\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 31/40: 100%|██████████| 26/26 [00:28<00:00,  1.11s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 31/40, Loss: 0.2568, Training Accuracy: 0.9179, Training F1-score: 0.9033, Training AUC-ROC: 0.9695\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation Epoch 31/40: 100%|██████████| 7/7 [00:05<00:00,  1.35it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 31/40, Validation Accuracy: 0.8416, Validation F1-score: 0.8185, Validation AUC-ROC: 0.9217\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 32/40: 100%|██████████| 26/26 [00:29<00:00,  1.13s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 32/40, Loss: 0.2280, Training Accuracy: 0.9080, Training F1-score: 0.8921, Training AUC-ROC: 0.9835\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation Epoch 32/40: 100%|██████████| 7/7 [00:05<00:00,  1.22it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 32/40, Validation Accuracy: 0.8416, Validation F1-score: 0.8185, Validation AUC-ROC: 0.9299\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 33/40: 100%|██████████| 26/26 [00:28<00:00,  1.11s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 33/40, Loss: 0.2783, Training Accuracy: 0.8955, Training F1-score: 0.8760, Training AUC-ROC: 0.9657\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation Epoch 33/40: 100%|██████████| 7/7 [00:08<00:00,  1.25s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 33/40, Validation Accuracy: 0.8317, Validation F1-score: 0.8066, Validation AUC-ROC: 0.9283\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 34/40: 100%|██████████| 26/26 [00:33<00:00,  1.30s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 34/40, Loss: 0.1766, Training Accuracy: 0.9403, Training F1-score: 0.9297, Training AUC-ROC: 0.9885\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation Epoch 34/40: 100%|██████████| 7/7 [00:06<00:00,  1.11it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 34/40, Validation Accuracy: 0.8515, Validation F1-score: 0.8304, Validation AUC-ROC: 0.9267\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 35/40: 100%|██████████| 26/26 [00:28<00:00,  1.11s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 35/40, Loss: 0.1720, Training Accuracy: 0.9478, Training F1-score: 0.9396, Training AUC-ROC: 0.9865\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation Epoch 35/40: 100%|██████████| 7/7 [00:05<00:00,  1.32it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 35/40, Validation Accuracy: 0.8515, Validation F1-score: 0.8292, Validation AUC-ROC: 0.9335\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 36/40: 100%|██████████| 26/26 [00:30<00:00,  1.16s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 36/40, Loss: 0.1682, Training Accuracy: 0.9502, Training F1-score: 0.9414, Training AUC-ROC: 0.9871\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation Epoch 36/40: 100%|██████████| 7/7 [00:05<00:00,  1.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 36/40, Validation Accuracy: 0.8416, Validation F1-score: 0.8177, Validation AUC-ROC: 0.9328\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 37/40: 100%|██████████| 26/26 [00:28<00:00,  1.10s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 37/40, Loss: 0.1543, Training Accuracy: 0.9453, Training F1-score: 0.9357, Training AUC-ROC: 0.9903\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation Epoch 37/40: 100%|██████████| 7/7 [00:06<00:00,  1.16it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 37/40, Validation Accuracy: 0.8416, Validation F1-score: 0.8183, Validation AUC-ROC: 0.9286\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 38/40: 100%|██████████| 26/26 [00:28<00:00,  1.11s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 38/40, Loss: 0.1981, Training Accuracy: 0.9403, Training F1-score: 0.9305, Training AUC-ROC: 0.9852\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation Epoch 38/40: 100%|██████████| 7/7 [00:05<00:00,  1.35it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 38/40, Validation Accuracy: 0.8515, Validation F1-score: 0.8304, Validation AUC-ROC: 0.9329\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 39/40: 100%|██████████| 26/26 [00:28<00:00,  1.11s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 39/40, Loss: 0.1520, Training Accuracy: 0.9403, Training F1-score: 0.9301, Training AUC-ROC: 0.9922\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation Epoch 39/40: 100%|██████████| 7/7 [00:06<00:00,  1.12it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 39/40, Validation Accuracy: 0.8218, Validation F1-score: 0.7937, Validation AUC-ROC: 0.9355\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 40/40: 100%|██████████| 26/26 [00:28<00:00,  1.09s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 40/40, Loss: 0.1561, Training Accuracy: 0.9527, Training F1-score: 0.9440, Training AUC-ROC: 0.9882\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation Epoch 40/40: 100%|██████████| 7/7 [00:05<00:00,  1.35it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 40/40, Validation Accuracy: 0.8614, Validation F1-score: 0.8416, Validation AUC-ROC: 0.9319\n",
            "\n",
            "Best Model Statistics:\n",
            "Best Validation Accuracy: 0.8614\n",
            "Best Validation F1-score: 0.8421\n",
            "Best Validation AUC-ROC: 0.9315\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 600x600 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}